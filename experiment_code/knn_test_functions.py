import os
import glob
import numpy as np
import scipy.io as spi
from scipy import stats
from sklearn import neighbors
import random
import pdb

def load_knn_data(data_fname,labelset):
    """load_knn_data(data_fname,labelset)
    Loads data from matlab files generated by the evlabel functions.

    Arguments:
        data_fname -- name of a "ftr_cell" .mat file generated by concat_ftrs.m
        labelset -- list of chars, e.g. ["i","a","b","c"]. Labels given to
                    syllables in song files. This function filters out any labels
                    from data_fname that are not found in labelset, as well as the
                    corresponding samples.

    Returns:
        samples -- m-by-n numpy array with m rows of samples, each having n features
        labels -- 1-D numpy array of length m, where each element is a label corresponding
                  to a row in 'samples'
        song_IDs -- 1-D numpy array of length m, where each element is the identity of the
                   song that the sample comes from. E.g., if elements 17:53 == 3, they are
                   all from song 3
    """
    FEATURES_TO_USE = np.r_[0:6,7:10,11] # using np._r to build index array 
    ftr_file = spi.loadmat(data_fname,chars_as_strings=False)
    #loads 'cell array' from .mat file. ftr_file is a dictionary of numpy record arrays
    #the 'feature_cell' record array has two columns: col 1 = actual vals, col 0 is just ftr names
    samples = np.hstack(ftr_file['feature_cell'][1,:]) # concatenate features horizontally (each row is a sample)
    samples = samples[:,FEATURES_TO_USE]; #discard unused features
    labels = ftr_file['labels'].flatten()
    labels = labels.view(np.uint32) # convert from unicode to long
    samples_to_keep = np.in1d(labels,labelset) #returns boolean vector, True where label is in labelset
    labels = labels[samples_to_keep]    
    samples = samples[samples_to_keep,:]
    samples = stats.zscore(samples)
    song_IDs = ftr_file['song_IDs'].flatten()[samples_to_keep]
    

    return samples, labels, song_IDs

def find_best_k(train_samples,train_labels,holdout_samples,holdout_labels):
    """find_best_k(train_samples,train_labels,holdout_samples,holdout_labels)
    Estimates accuracy of k-neearest neighbors algorithm using different values
    of k on the samples in data_fname. As currently written, this function loops
    from k=1 to k=10. For each value of k, it generates 10 replicates by using
    a random two-thirds of the data as a training set and then using the other
    third as the validation set.

    Note that the algorithm uses the distances weighted by 
    their inverse to determine the nearest neighbor, since I found empirically
    that the weighted distances always give slightly better accuracy.
    
    Arguments:
        train_samples -- m-by-n numpy array with m rows of samples, each having n features
        train_labels -- numpy vector of length m, where each element is a label corresponding
                  to a row in 'samples'
        holdout_samples, holdout_labels -- same as train_samples and train_labels except this
                                           is the set kept separate and used to find best hyper-
                                           -parameters
        
    Returns:
        mn_scores -- vector of mean scores for each value of k
        best_k -- value of k corresponding to max value in 'scores'

    """

    # test loop
    num_nabes_list = range(1,10,1)
    scores = np.empty((10,))
    for ind, num_nabes in enumerate(num_nabes_list):
        clf = neighbors.KNeighborsClassifier(num_nabes,'distance')
        clf.fit(train_samples,train_labels)
        scores[ind] = clf.score(holdout_samples,holdout_labels)
    k = num_nabes_list[scores.argmax()] #argmax returns index of max val
    print("best k was {} with accuracy of {}".format(k,np.max(scores)))
    return scores, k

def knn_test(train_fname,test_fname,K,labelset):
    """knn_test(train_fname,test_fname,K,labelset)
    Fits a k-nearest neighbors classifier with training data, returns the accuracy
    when using the the fitted classifier to predict labels in the testing data.

    Arguments:
        train_fname -- string, name of file containing samples used to train KNN classifier
        test_fname -- string, name of file containing samples used to test classifier
        K -- integer, number of neighbors to use to classify
        labelset -- list of chars, e.g. ["i","a","b","c"]. Labels

    Returns:
        Score -- accuracy as computed by SciKitLearn KNeighborsClassifier. It is
                 simply the number of correctly classified samples over the total
                 number of samples. 
    """
    clf = neighbors.KNeighborsClassifier(K,'distance')
    
    train_samples, train_labels = load_data(train_fname,labelset)
    clf.fit(train_samples,train_labels)    

    test_samples, test_labels = load_data(test_fname,labelset)
    return clf.score(test_samples,test_labels)
